- id: azureml://registries/azureml-ai21/models/AI21-Jamba-Instruct/versions/2
  registry: azureml-ai21
  name: AI21-Jamba-Instruct
  original_name: AI21-Jamba-Instruct
  friendly_name: AI21-Jamba-Instruct
  task: chat-completion
  publisher: AI21 Labs
  license: custom
  description: |
    Jamba-Instruct is the world's first production-grade Mamba-based LLM model and leverages its hybrid Mamba-Transformer architecture to achieve best-in-class performance, quality, and cost efficiency.

    **Model Developer Name**: _AI21 Labs_

    ## Model Architecture

    Jamba-Instruct leverages a hybrid Mamba-Transformer architecture to achieve best-in-class performance, quality, and cost efficiency.
    AI21's Jamba architecture features a blocks-and-layers approach that allows Jamba to successfully integrate the two architectures. Each Jamba block contains either an attention or a Mamba layer, followed by a multi-layer perceptron (MLP), producing an overall ratio of one Transformer layer out of every eight total layers.
  samples: null
  summary: Jamba-Instruct is the world's first production-grade Mamba-based LLM model and leverages its hybrid Mamba-Transformer architecture to achieve best-in-class performance, quality, and cost efficiency.
  model_family: AI21 Labs
  model_version: 2
  notes: ""
  tags:
    - chat
    - rag
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/ai21 labs.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-cohere/models/Cohere-command-r/versions/3
  registry: azureml-cohere
  name: Cohere-command-r
  original_name: Cohere-command-r
  friendly_name: Cohere Command R
  task: chat-completion
  publisher: cohere
  license: custom
  description: "Command R is a highly performant generative large language model, optimized for a variety of use cases including reasoning, summarization, and question answering. \n\nThe model is optimized to perform well in the following languages: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Simplified Chinese, and Arabic.\n\nPre-training data additionally included the following 13 languages: Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, Persian.\n\n## Resources\n\nFor full details of this model, [release blog post](https://aka.ms/cohere-blog).\n\n## Model Architecture\n\nThis is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety.\n\n### Tool use capabilities\n\nCommand R has been specifically trained with conversational tool use capabilities. These have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template will likely reduce performance, but we encourage experimentation.\n\nCommand R's tool use functionality takes a conversation as input (with an optional user-system preamble), along with a list of available tools. The model will then generate a json-formatted list of actions to execute on a subset of those tools. Command R may use one of its supplied tools more than once.\n\nThe model has been trained to recognise a special directly_answer tool, which it uses to indicate that it doesn't want to use any of its other tools. The ability to abstain from calling a specific tool can be useful in a range of situations, such as greeting a user, or asking clarifying questions. We recommend including the directly_answer tool, but it can be removed or renamed if required.\n\n### Grounded Generation and RAG Capabilities\n\nCommand R has been specifically trained with grounded generation capabilities. This means that it can generate responses based on a list of supplied document snippets, and it will include grounding spans (citations) in its response indicating the source of the information. This can be used to enable behaviors such as grounded summarization and the final step of Retrieval Augmented Generation (RAG).This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template may reduce performance, but we encourage experimentation.\n\nCommand R's grounded generation behavior takes a conversation as input (with an optional user-supplied system preamble, indicating task, context and desired output style), along with a list of retrieved document snippets. The document snippets should be chunks, rather than long documents, typically around 100-400 words per chunk. Document snippets consist of key-value pairs. The keys should be short descriptive strings, the values can be text or semi-structured.\n\nBy default, Command R will generate grounded responses by first predicting which documents are relevant, then predicting which ones it will cite, then generating an answer. Finally, it will then insert grounding spans into the answer. See below for an example. This is referred to as accurate grounded generation.\n\nThe model is trained with a number of other answering modes, which can be selected by prompt changes . A fast citation mode is supported in the tokenizer, which will directly generate an answer with grounding spans in it, without first writing the answer out in full. This sacrifices some grounding accuracy in favor of generating fewer tokens.\n\n### Code Capabilities\n\nCommand R has been optimized to interact with your code, by requesting code snippets, code explanations, or code rewrites. It might not perform well out-of-the-box for pure code completion. For better performance, we also recommend using a low temperature (and even greedy decoding) for code-generation related instructions.\n"
  samples: null
  summary: Command R is a scalable generative model targeting RAG and Tool Use to enable production-scale AI for enterprise.
  model_family: cohere
  model_version: 3
  notes: ""
  tags:
    - rag
    - multilingual
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/cohere.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-cohere/models/Cohere-command-r-plus/versions/3
  registry: azureml-cohere
  name: Cohere-command-r-plus
  original_name: Cohere-command-r-plus
  friendly_name: Cohere Command R+
  task: chat-completion
  publisher: cohere
  license: custom
  description: "Command R+ is a highly performant generative large language model, optimized for a variety of use cases including reasoning, summarization, and question answering. \n\nThe model is optimized to perform well in the following languages: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Simplified Chinese, and Arabic.\n\nPre-training data additionally included the following 13 languages: Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, Persian.\n\n## Resources\n\nFor full details of this model, [release blog post](https://aka.ms/cohere-blog).\n\n## Model Architecture\n\nThis is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety.\n\n### Tool use capabilities\n\nCommand R+ has been specifically trained with conversational tool use capabilities. These have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template will likely reduce performance, but we encourage experimentation.\n\nCommand R+'s tool use functionality takes a conversation as input (with an optional user-system preamble), along with a list of available tools. The model will then generate a json-formatted list of actions to execute on a subset of those tools. Command R+ may use one of its supplied tools more than once.\n\nThe model has been trained to recognise a special directly_answer tool, which it uses to indicate that it doesn't want to use any of its other tools. The ability to abstain from calling a specific tool can be useful in a range of situations, such as greeting a user, or asking clarifying questions. We recommend including the directly_answer tool, but it can be removed or renamed if required.\n\n### Grounded Generation and RAG Capabilities\n\nCommand R+ has been specifically trained with grounded generation capabilities. This means that it can generate responses based on a list of supplied document snippets, and it will include grounding spans (citations) in its response indicating the source of the information. This can be used to enable behaviors such as grounded summarization and the final step of Retrieval Augmented Generation (RAG).This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template may reduce performance, but we encourage experimentation.\n\nCommand R+'s grounded generation behavior takes a conversation as input (with an optional user-supplied system preamble, indicating task, context and desired output style), along with a list of retrieved document snippets. The document snippets should be chunks, rather than long documents, typically around 100-400 words per chunk. Document snippets consist of key-value pairs. The keys should be short descriptive strings, the values can be text or semi-structured.\n\nBy default, Command R+ will generate grounded responses by first predicting which documents are relevant, then predicting which ones it will cite, then generating an answer. Finally, it will then insert grounding spans into the answer. See below for an example. This is referred to as accurate grounded generation.\n\nThe model is trained with a number of other answering modes, which can be selected by prompt changes . A fast citation mode is supported in the tokenizer, which will directly generate an answer with grounding spans in it, without first writing the answer out in full. This sacrifices some grounding accuracy in favor of generating fewer tokens.\n\n### Code Capabilities\n\nCommand R+ has been optimized to interact with your code, by requesting code snippets, code explanations, or code rewrites. It might not perform well out-of-the-box for pure code completion. For better performance, we also recommend using a low temperature (and even greedy decoding) for code-generation related instructions.\n"
  samples: null
  summary: Command R+ is a state-of-the-art RAG-optimized model designed to tackle enterprise-grade workloads.
  model_family: cohere
  model_version: 3
  notes: ""
  tags:
    - rag
    - multilingual
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/cohere.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-cohere/models/Cohere-embed-v3-english/versions/3
  registry: azureml-cohere
  name: Cohere-embed-v3-english
  original_name: Cohere-embed-v3-english
  friendly_name: Cohere Embed v3 English
  task: embeddings
  publisher: cohere
  license: custom
  description: Cohere Embed English is the market's leading text representation model used for semantic search, retrieval-augmented generation (RAG), classification, and clustering. Embed English has top performance on the HuggingFace MTEB benchmark and performs well on a variety of industries such as Finance, Legal, and General-Purpose Corpora.The model was trained on nearly 1B English training pairs. For full details of this model, [release blog post](https://aka.ms/cohere-blog).
  samples: null
  summary: Cohere Embed English is the market's leading text representation model used for semantic search, retrieval-augmented generation (RAG), classification, and clustering.
  model_family: cohere
  model_version: 3
  notes: ""
  tags:
    - RAG
    - search
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/cohere.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-cohere/models/Cohere-embed-v3-multilingual/versions/3
  registry: azureml-cohere
  name: Cohere-embed-v3-multilingual
  original_name: Cohere-embed-v3-multilingual
  friendly_name: Cohere Embed v3 Multilingual
  task: embeddings
  publisher: cohere
  license: custom
  description: Cohere Embed Multilingual is the market's leading text representation model used for semantic search, retrieval-augmented generation (RAG), classification, and clustering. Embed Multilingual supports 100+ languages and can be used to search within a language (e.g., search with a French query on French documents) and across languages (e.g., search with an English query on Chinese documents). This model was trained on nearly 1B English training pairs and nearly 0.5B Non-English training pairs from 100+ languages. For full details of this model, [release blog post](https://aka.ms/cohere-blog).
  samples: null
  summary: Supporting over 100 languages, Cohere Embed Multilingual is the market's leading text representation model used for semantic search, retrieval-augmented generation (RAG), classification, and clustering.
  model_family: cohere
  model_version: 3
  notes: ""
  tags:
    - RAG
    - search
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/cohere.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-meta/models/Meta-Llama-3-70B-Instruct/versions/6
  registry: azureml-meta
  name: Meta-Llama-3-70B-Instruct
  original_name: Meta-Llama-3-70B-Instruct
  friendly_name: Meta-Llama-3-70B-Instruct
  task: chat-completion
  publisher: meta
  license: custom
  description: "Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. \n\n## Model Architecture\n\nLlama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n## Training Datasets\n\n**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of March 2023 for the 8B and December 2023 for the 70B models respectively. \n"
  samples: null
  summary: A powerful 70-billion parameter model excelling in reasoning, coding, and broad language applications.
  model_family: meta
  model_version: 6
  notes: ""
  tags:
    - conversation
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/meta.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-meta/models/Meta-Llama-3-8B-Instruct/versions/6
  registry: azureml-meta
  name: Meta-Llama-3-8B-Instruct
  original_name: Meta-Llama-3-8B-Instruct
  friendly_name: Meta-Llama-3-8B-Instruct
  task: chat-completion
  publisher: meta
  license: custom
  description: "Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. \n\n## Model Architecture\n\nLlama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n## Training Datasets\n\n**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of March 2023 for the 8B and December 2023 for the 70B models respectively. \n"
  samples: null
  summary: A versatile 8-billion parameter model optimized for dialogue and text generation tasks.
  model_family: meta
  model_version: 6
  notes: ""
  tags:
    - conversation
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/meta.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-meta/models/Meta-Llama-3.1-405B-Instruct/versions/1
  registry: azureml-meta
  name: Meta-Llama-3-1-405B-Instruct
  original_name: Meta-Llama-3.1-405B-Instruct
  friendly_name: Meta-Llama-3.1-405B-Instruct
  task: chat-completion
  publisher: meta
  license: custom
  description: |
    The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned
    generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on
    common industry benchmarks.

    ## Model Architecture

    Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.

    ## Training Datasets

    **Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.

    **Data Freshness:** The pretraining data has a cutoff of December 2023.
  samples: null
  summary: The Llama 3.1 instruction tuned text only models are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.
  model_family: meta
  model_version: 1
  notes: ""
  tags:
    - conversation
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/meta.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-meta/models/Meta-Llama-3.1-70B-Instruct/versions/1
  registry: azureml-meta
  name: Meta-Llama-3-1-70B-Instruct
  original_name: Meta-Llama-3.1-70B-Instruct
  friendly_name: Meta-Llama-3.1-70B-Instruct
  task: chat-completion
  publisher: meta
  license: custom
  description: |
    The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned
    generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on
    common industry benchmarks.

    ## Model Architecture

    Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.

    ## Training Datasets

    **Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.

    **Data Freshness:** The pretraining data has a cutoff of December 2023.
  samples: null
  summary: The Llama 3.1 instruction tuned text only models are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.
  model_family: meta
  model_version: 1
  notes: ""
  tags:
    - conversation
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/meta.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-meta/models/Meta-Llama-3.1-8B-Instruct/versions/1
  registry: azureml-meta
  name: Meta-Llama-3-1-8B-Instruct
  original_name: Meta-Llama-3.1-8B-Instruct
  friendly_name: Meta-Llama-3.1-8B-Instruct
  task: chat-completion
  publisher: meta
  license: custom
  description: |
    The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned
    generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on
    common industry benchmarks.

    ## Model Architecture

    Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.

    ## Training Datasets

    **Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.

    **Data Freshness:** The pretraining data has a cutoff of December 2023.
  samples: null
  summary: The Llama 3.1 instruction tuned text only models are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.
  model_family: meta
  model_version: 1
  notes: ""
  tags:
    - conversation
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/meta.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-mistral/models/Mistral-large/versions/1
  registry: azureml-mistral
  name: Mistral-large
  original_name: Mistral-large
  friendly_name: Mistral Large
  task: chat-completion
  publisher: mistralai
  license: custom
  description: |
    Mistral Large is Mistral AI's most advanced Large Language Model (LLM). It can be used on any language-based task thanks to its state-of-the-art reasoning and knowledge capabilities.

    Additionally, Mistral Large is:

    - **Specialized in RAG.** Crucial information is not lost in the middle of long context windows (up to 32K tokens).
    - **Strong in coding.**  Code generation, review and comments. Supports all mainstream coding languages.
    - **Multi-lingual by design.** Best-in-class performance in French, German, Spanish, and Italian - in addition to English. Dozens of other languages are supported.
    - **Responsible AI.** Efficient guardrails baked in the model, with additional safety layer with safe_mode option

    ## Resources

    For full details of this model, please read [release blog post](https://aka.ms/mistral-blog).
  samples: null
  summary: Mistral's flagship model that's ideal for complex tasks that require large reasoning capabilities or are highly specialized (Synthetic Text Generation, Code Generation, RAG, or Agents).
  model_family: mistralai
  model_version: 1
  notes: ""
  tags:
    - reasoning
    - rag
    - agents
    - multilingual
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/mistralai.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-mistral/models/Mistral-large-2407/versions/1
  registry: azureml-mistral
  name: Mistral-large-2407
  original_name: Mistral-large-2407
  friendly_name: Mistral Large (2407)
  task: chat-completion
  publisher: mistralai
  license: custom
  description: "Mistral Large (2407) is an advanced Large Language Model (LLM) with state-of-the-art reasoning, knowledge and coding capabilities.\n\n**Multi-lingual by design.** Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish\n\n**Proficient in coding.** Trained on 80+ coding languages such as Python, Java, C, C++, JavaScript, and Bash. Also trained on more specific languages such as Swift and Fortran\n\n**Agent-centric.** Best-in-class agentic capabilities with native function calling and JSON outputting \n\n**Advanced Reasoning.** State-of-the-art mathematical and reasoning capabilities\n"
  samples: null
  summary: Mistral Large (2407) is an advanced Large Language Model (LLM) with state-of-the-art reasoning, knowledge and coding capabilities.
  model_family: mistralai
  model_version: 1
  notes: ""
  tags:
    - reasoning
    - rag
    - agents
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/mistralai.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-mistral/models/Mistral-Nemo/versions/1
  registry: azureml-mistral
  name: Mistral-Nemo
  original_name: Mistral-Nemo
  friendly_name: Mistral Nemo
  task: chat-completion
  publisher: mistralai
  license: custom
  description: |
    Mistral Nemo is a cutting-edge Language Model (LLM) boasting state-of-the-art reasoning, world knowledge, and coding capabilities within its size category.

    **Jointly developed with Nvidia.** This collaboration has resulted in a powerful 12B model that pushes the boundaries of language understanding and generation.

    **Multilingual proficiency.** Mistral Nemo is equipped with a new tokenizer, Tekken, designed for multilingual applications. It supports over 100 languages, including but not limited to English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, Polish, and many more. Tekken has proven to be more efficient than the Llama 3 tokenizer in compressing text for approximately 85% of all languages, with significant improvements in Malayalam, Hindi, Arabic, and prevalent European languages.

    **Agent-centric.** Mistral Nemo possesses top-tier agentic capabilities, including native function calling and JSON outputting.

    **Advanced Reasoning.** Mistral Nemo demonstrates state-of-the-art mathematical and reasoning capabilities within its size category.
  samples: null
  summary: Mistral Nemo is a cutting-edge Language Model (LLM) boasting state-of-the-art reasoning, world knowledge, and coding capabilities within its size category.
  model_family: mistralai
  model_version: 1
  notes: ""
  tags:
    - reasoning
    - rag
    - agents
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/mistralai.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml-mistral/models/Mistral-small/versions/1
  registry: azureml-mistral
  name: Mistral-small
  original_name: Mistral-small
  friendly_name: Mistral Small
  task: chat-completion
  publisher: mistralai
  license: custom
  description: "Mistral Small is Mistral AI's most efficient Large Language Model (LLM). It can be used on any language-based task that requires high efficiency and low latency.\n\nMistral Small is:\n\n- **A small model optimized for low latency.** Very efficient for high volume and low latency workloads. Mistral Small is Mistral's smallest proprietary model, it outperforms Mixtral 8x7B and has lower latency. \n- **Specialized in RAG.** Crucial information is not lost in the middle of long context windows (up to 32K tokens).\n- **Strong in coding.** Code generation, review and comments. Supports all mainstream coding languages.\n- **Multi-lingual by design.** Best-in-class performance in French, German, Spanish, and Italian - in addition to English. Dozens of other languages are supported.\n- **Responsible AI.** Efficient guardrails baked in the model, with additional safety layer with safe_mode option\n\n## Resources\n\nFor full details of this model, please read [release blog post](https://aka.ms/mistral-blog).\n"
  samples: null
  summary: Mistral Small can be used on any language-based task that requires high efficiency and low latency.
  model_family: mistralai
  model_version: 1
  notes: ""
  tags:
    - low latency
    - multilingual
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/mistralai.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azure-openai/models/gpt-4o/versions/2
  registry: azure-openai
  name: gpt-4o
  original_name: gpt-4o
  friendly_name: OpenAI GPT-4o
  task: chat-completion
  publisher: Azure OpenAI Service
  license: custom
  description: "GPT-4o offers a shift in how AI models interact with multimodal inputs. By seamlessly combining text, images, and audio, GPT-4o provides a richer, more engaging user experience.\n\nMatching the intelligence of GPT-4 Turbo, it is remarkably more efficient, delivering text at twice the speed and at half the cost. Additionally, GPT-4o exhibits the highest vision performance and excels in non-English languages compared to previous OpenAI models.\n\nGPT-4o is engineered for speed and efficiency. Its advanced ability to handle complex queries with minimal resources can translate into cost savings and performance.\n\nThe introduction of GPT-4o opens numerous possibilities for businesses in various sectors: \n\n1. **Enhanced customer service**: By integrating diverse data inputs, GPT-4o enables more dynamic and comprehensive customer support interactions.\n2. **Advanced analytics**: Leverage GPT-4o's capability to process and analyze different types of data to enhance decision-making and uncover deeper insights.\n3. **Content innovation**: Use GPT-4o's generative capabilities to create engaging and diverse content formats, catering to a broad range of consumer preferences.\n\n## Resources\n\n- [\"Hello GPT-4o\" (OpenAI announcement)](https://openai.com/index/hello-gpt-4o/)\n- [Introducing GPT-4o: OpenAI's new flagship multimodal model now in preview on Azure](https://azure.microsoft.com/en-us/blog/introducing-gpt-4o-openais-new-flagship-multimodal-model-now-in-preview-on-azure/)\n"
  samples: null
  summary: OpenAI's most advanced multimodal model in the GPT-4 family. Can handle both text and image inputs.
  model_family: openai
  model_version: 2
  notes: ""
  tags:
    - multipurpose
    - multilingual
    - multimodal
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/openai.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azure-openai/models/gpt-4o-mini/versions/1
  registry: azure-openai
  name: gpt-4o-mini
  original_name: gpt-4o-mini
  friendly_name: OpenAI GPT-4o mini
  task: chat-completion
  publisher: Azure OpenAI Service
  license: custom
  description: |
    GPT-4o mini enables a broad range of tasks with its low cost and latency, such as applications that chain or parallelize multiple model calls (e.g., calling multiple APIs), pass a large volume of context to the model (e.g., full code base or conversation history), or interact with customers through fast, real-time text responses (e.g., customer support chatbots).

    Today, GPT-4o mini supports text and vision in the API, with support for text, image, video and audio inputs and outputs coming in the future. The model has a context window of 128K tokens and knowledge up to October 2023. Thanks to the improved tokenizer shared with GPT-4o, handling non-English text is now even more cost effective.

    GPT-4o mini surpasses GPT-3.5 Turbo and other small models on academic benchmarks across both textual intelligence and multimodal reasoning, and supports the same range of languages as GPT-4o. It also demonstrates strong performance in function calling, which can enable developers to build applications that fetch data or take actions with external systems, and improved long-context performance compared to GPT-3.5 Turbo.

    ## Resources

    - [OpenAI announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)
  samples: null
  summary: An affordable, efficient AI solution for diverse text and image tasks.
  model_family: OpenAI
  model_version: 1
  notes: ""
  tags:
    - multipurpose
    - multilingual
    - multimodal
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/openai.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azure-openai/models/text-embedding-3-large/versions/1
  registry: azure-openai
  name: text-embedding-3-large
  original_name: text-embedding-3-large
  friendly_name: OpenAI Text Embedding 3 (large)
  task: embeddings
  publisher: Azure OpenAI Service
  license: custom
  description: Text-embedding-3 series models are the latest and most capable embedding model. The text-embedding-3 models offer better average multi-language retrieval performance with the MIRACL benchmark while still maintaining performance for English tasks with the MTEB benchmark.
  samples: null
  summary: Text-embedding-3 series models are the latest and most capable embedding model from OpenAI.
  model_family: openai
  model_version: 1
  notes: ""
  tags:
    - RAG
    - search
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/openai.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azure-openai/models/text-embedding-3-small/versions/1
  registry: azure-openai
  name: text-embedding-3-small
  original_name: text-embedding-3-small
  friendly_name: OpenAI Text Embedding 3 (small)
  task: embeddings
  publisher: Azure OpenAI Service
  license: custom
  description: Text-embedding-3 series models are the latest and most capable embedding model. The text-embedding-3 models offer better average multi-language retrieval performance with the MIRACL benchmark while still maintaining performance for English tasks with the MTEB benchmark.
  samples: null
  summary: Text-embedding-3 series models are the latest and most capable embedding model from OpenAI.
  model_family: openai
  model_version: 1
  notes: ""
  tags:
    - RAG
    - search
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/openai.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml/models/Phi-3-medium-128k-instruct/versions/3
  registry: azureml
  name: Phi-3-medium-128k-instruct
  original_name: Phi-3-medium-128k-instruct
  friendly_name: Phi-3-medium instruct (128k)
  task: chat-completion
  publisher: microsoft
  license: mit
  description: "The Phi-3-Medium-128K-Instruct is a 14B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Medium version in two variants 4k and 128K which is the context length (in tokens) that it can support.\n\nThe model underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3-Medium-128K-Instruct showcased a robust and state-of-the-art performance among models of the same-size and next-size-up.\n\n## Resources\n\n\U0001F3E1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\U0001F4F0 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n\U0001F4D6 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n\U0001F6E0️ [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n\U0001F469‍\U0001F373 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n## Model Architecture\n\nPhi-3-Medium-128k-Instruct has 14B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n## Training Datasets\n\nOur training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, \"textbook - like\" data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).\n"
  samples: null
  summary: Same model as Phi-3-medium (4k) but with larger context size. Use this for RAG or few shot prompting.
  model_family: microsoft
  model_version: 3
  notes: ""
  tags:
    - reasoning
    - understanding
    - large context
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/microsoft.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml/models/Phi-3-medium-4k-instruct/versions/3
  registry: azureml
  name: Phi-3-medium-4k-instruct
  original_name: Phi-3-medium-4k-instruct
  friendly_name: Phi-3-medium instruct (4k)
  task: chat-completion
  publisher: microsoft
  license: mit
  description: "The Phi-3-Medium-4K-Instruct is a 14B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Medium version in two variants 4K and 128K which is the context length (in tokens) that it can support.\n\nThe model underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3-Medium-4K-Instruct showcased a robust and state-of-the-art performance among models of the same-size and next-size-up.\n\n## Resources\n\n\U0001F3E1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\U0001F4F0 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n\U0001F4D6 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n\U0001F6E0️ [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n\U0001F469‍\U0001F373 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n## Model Architecture\n\nPhi-3-Medium-4K-Instruct has 14B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n## Training Datasets\n\nOur training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, \"textbook-like\" data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).\n"
  samples: null
  summary: A 14B parameter model. Use this larger model for better quality than Phi-3-mini, and with reasoning-dense data.
  model_family: microsoft
  model_version: 3
  notes: ""
  tags:
    - reasoning
    - understanding
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/microsoft.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml/models/Phi-3-mini-128k-instruct/versions/10
  registry: azureml
  name: Phi-3-mini-128k-instruct
  original_name: Phi-3-mini-128k-instruct
  friendly_name: Phi-3-mini instruct (128k)
  task: chat-completion
  publisher: microsoft
  license: mit
  description: "The Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.\nThis dataset includes both synthetic data and filtered publicly available website data, with an emphasis on high-quality and reasoning-dense properties.\n\nAfter initial training, the model underwent a post-training process that involved supervised fine-tuning and direct preference optimization to enhance its ability to follow instructions and adhere to safety measures.\nWhen evaluated against benchmarks that test common sense, language understanding, mathematics, coding, long-term context, and logical reasoning, the Phi-3 Mini-128K-Instruct demonstrated robust and state-of-the-art performance among models with fewer than 13 billion parameters.\n\n## Resources\n\n\U0001F3E1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\U0001F4F0 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n\U0001F4D6 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n\U0001F6E0️ [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n\U0001F469‍\U0001F373 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n## Model Architecture\n\nPhi-3 Mini-128K-Instruct has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n## Training Datasets\n\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, \"textbook - like\" data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).\n"
  samples: null
  summary: Same model as Phi-3-mini (4k) but with larger context size. Use this for RAG or few shot prompting.
  model_family: microsoft
  model_version: 10
  notes: ""
  tags:
    - reasoning
    - understanding
    - low latency
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/microsoft.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml/models/Phi-3-mini-4k-instruct/versions/10
  registry: azureml
  name: Phi-3-mini-4k-instruct
  original_name: Phi-3-mini-4k-instruct
  friendly_name: Phi-3-mini instruct (4k)
  task: chat-completion
  publisher: microsoft
  license: mit
  description: "The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.\n\nThe model underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\n\n## Resources\n\n\U0001F3E1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\U0001F4F0 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n\U0001F4D6 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n\U0001F6E0️ [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n\U0001F469‍\U0001F373 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n## Model Architecture\n\nPhi-3 Mini-4K-Instruct has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n## Training Datasets\n\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, \"textbook - like\" data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).\n"
  samples: null
  summary: Tiniest Phi-3 model. Optimized for both quality and low latency.
  model_family: microsoft
  model_version: 10
  notes: ""
  tags:
    - reasoning
    - understanding
    - low latency
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/microsoft.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml/models/Phi-3-small-128k-instruct/versions/3
  registry: azureml
  name: Phi-3-small-128k-instruct
  original_name: Phi-3-small-128k-instruct
  friendly_name: Phi-3-small instruct (128k)
  task: chat-completion
  publisher: microsoft
  license: mit
  description: "The Phi-3-Small-128K-Instruct is a 7B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model supports 128K context length (in tokens).\n\nThe model underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3-Small-128K-Instruct showcased a robust and state-of-the-art performance among models of the same-size and next-size-up.\n\n## Resources\n\n+ [Phi-3 Microsoft Blog](https://aka.ms/phi3blog-april)\n+ [Phi-3 Technical Report](https://aka.ms/phi3-tech-report)\n\n## Model Architecture\n\nPhi-3 Small-128K-Instruct has 7B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n## Training Datasets\n\nOur training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report)."
  samples: null
  summary: Same Phi-3-small model, but with a larger context size for RAG or few shot prompting.
  model_family: microsoft
  model_version: 3
  notes: ""
  tags:
    - reasoning
    - understanding
    - large context
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/microsoft.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml/models/Phi-3-small-8k-instruct/versions/3
  registry: azureml
  name: Phi-3-small-8k-instruct
  original_name: Phi-3-small-8k-instruct
  friendly_name: Phi-3-small instruct (8k)
  task: chat-completion
  publisher: microsoft
  license: mit
  description: "The Phi-3-Small-8K-Instruct is a 7B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model supports 8K context length (in tokens).\n\nThe model underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3-Small-8K-Instruct showcased a robust and state-of-the-art performance among models of the same-size and next-size-up.\n\n## Resources\n\n\U0001F3E1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\U0001F4F0 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n\U0001F4D6 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n\U0001F6E0️ [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n\U0001F469‍\U0001F373 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n## Model Architecture\n\nPhi-3 Small-8K-Instruct has 7B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n## Training Datasets\n\nOur training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report)."
  samples: null
  summary: A 7B parameters model. Use this larger model for better quality than Phi-3-mini, and with reasoning-dense data.
  model_family: microsoft
  model_version: 3
  notes: ""
  tags:
    - reasoning
    - understanding
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/microsoft.svg
  evaluation: ""
  license_description: ""
- id: azureml://registries/azureml/models/Phi-3.5-mini-instruct/versions/2
  registry: azureml
  name: Phi-3-5-mini-instruct
  original_name: Phi-3.5-mini-instruct
  friendly_name: Phi-3.5-mini instruct (128k)
  task: chat-completion
  publisher: microsoft
  license: mit
  description: "Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\n### Resources\n\U0001F3E1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\U0001F4F0 [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>\n\U0001F4D6 [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>\n\U0001F469‍\U0001F373 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n### Model Architecture\nPhi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini. It is a text-only model best suited for prompts using chat format.\n\n### Training Data\nPhi-3.5-mini is a static model trained on an offline dataset with 3.4T tokens and a cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.\n"
  samples: null
  summary: Refresh of Phi-3-mini model
  model_family: microsoft
  model_version: 2
  notes: ""
  tags:
    - reasoning
    - understanding
    - low latency
  rate_limit_tier: null
  supported_languages: []
  max_output_tokens: null
  max_input_tokens: 0
  training_data_date: ""
  logo_url: /images/modules/marketplace/models/families/microsoft.svg
  evaluation: ""
  license_description: ""
